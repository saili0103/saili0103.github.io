@article{Tibshirani96,
abstract = {Document: Details (1994) Robert Tibshirani CiteSeer.IST - Copyright Penn State and NEC},
archivePrefix = {arXiv},
arxivId = {1369?7412/11/73273},
author = {Tibshirani, Robert},
doi = {10.2307/2346178},
eprint = {11/73273},
isbn = {0849320240},
issn = {00359246},
journal = {Journal of the Royal Statistical Society B},
mendeley-groups = {Debiased Lasso},
number = {1},
pages = {267--288},
pmid = {16272381},
primaryClass = {1369?7412},
title = {{Regression selection and shrinkage via the lasso}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.7574},
volume = {58},
year = {1996}
}

@article{CL13,
abstract = {Zou [J. Amer. Statist. Assoc. 101 (2006) 1418?1429] proposed the Adaptive LASSO (ALASSO) method for simultaneous variable selection and estimation of the regression parameters, and established its oracle property. In this paper, we investigate the rate of convergence of the ALASSO estimator to the oracle distribution when the dimension of the regression parameters may grow to infinity with the sample size. It is shown that the rate critically depends on the choices of the penalty parameter and the initial estimator, among other factors, and that confidence intervals (CIs) based on the oracle limit law often have poor coverage accuracy. As an alternative, we consider the residual bootstrap method for the ALASSO estimators that has been recently shown to be consistent; cf. Chatterjee and Lahiri [J. Amer. Statist. Assoc. 106 (2011a) 608?625]. We show that the bootstrap applied to a suitable studentized version of the ALASSO estimator achieves second-order correctness, even when the dimension of the regression parameters is unbounded. Results from a moderately large simulation study show marked improvement in coverage accuracy for the bootstrap CIs over the oracle based CIs.},
archivePrefix = {arXiv},
arxivId = {1307.1952},
author = {Chatterjee, A. and Lahiri, S. N.},
doi = {10.1214/13-AOS1106},
eprint = {1307.1952},
file = {:Users/saili/Dropbox/bootstrap/Bootstrap in high-dimensional problems/Lahiri-Adaptive Lasso.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bootstrap,Edgeworth expansion,Penalized regression},
mendeley-groups = {Debiased Lasso},
number = {3},
pages = {1232--1259},
title = {{Rates of convergence of the adaptive lasso estimators to the oracle distribution and higher order refinements by the bootstrap}},
volume = {41},
year = {2013}
}

@article{JM14,
author = {Javanmard, A.and Montanari, A.},
file = {:Users/saili/Dropbox/Papers1/High-dimensional/javanmard14a.pdf:pdf},
keywords = {bias of an estimator,confidence intervals,high-dimensional models,hypothesis testing,lasso},
pages = {2869--2909},
title = {{Confidence Intervals and Hypothesis Testing for High-Dimensional Regression}},
volume = {15},
year = {2014}
}

@article{ZZ14,
abstract = {The purpose of this paper is to propose methodologies for statistical inference of low-dimensional parameters with high-dimensional data. We focus on constructing confidence intervals for individual coefficients and linear combinations of several of them in a linear regression model, although our ideas are applicable in a much broad context. The theoretical results presented here provide sufficient conditions for the asymptotic normality of the proposed estimators along with a consistent estimator for their finite-dimensional covariance matrices. These sufficient conditions allow the number of variables to far exceed the sample size. The simulation results presented here demonstrate the accuracy of the coverage probability of the proposed confidence intervals, strongly supporting the theoretical results.},
archivePrefix = {arXiv},
arxivId = {arXiv:1110.2563v2},
author = {Zhang, C.-H. and Zhang, S.},
doi = {10.1111/rssb.12026},
eprint = {arXiv:1110.2563v2},
file = {:Users/saili/Dropbox/Papers1/High-dimensional/Zhang-Confidence intervals for low dimensional parameters in high dimensional linear models.pdf:pdf},
isbn = {1467-9868},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Confidence interval,High dimension,Linear regression model,Statistical inference,p-value},
number = {1},
pages = {217--242},
title = {{Confidence intervals for low dimensional parameters in high dimensional linear models}},
url = {http://doi.wiley.com/10.1111/rssb.12026},
volume = {76},
year = {2014}
}
@article{Van14,
abstract = {We propose a general method for constructing confidence intervals and statistical tests for single or low-dimensional components of a large parameter vector in a high-dimensional model. It can be easily adjusted for multiplicity taking dependence among tests into account. For linear models, our method is essentially the same as in Zhang and Zhang [J. R. Stat. Soc. Ser. B Stat. Methodol. 76 (2014) 217â€“242]: we analyze its asymptotic properties and establish its asymptotic op- timality in terms of semiparametric efficiency. Our method naturally extends to generalized linear models with convex loss functions. We develop the corresponding theory which includes a careful analysis for Gaussian, sub-Gaussian and bounded correlated designs. 1.},
archivePrefix = {arXiv},
arxivId = {arXiv:1303.0518v3},
author={Van de Geer, Sara and B{\"u}hlmann, Peter and Ritov, Ya?acov and Dezeure, Ruben},
 doi = {10.1214/14-AOS1221},
eprint = {arXiv:1303.0518v3},
file = {:Users/saili/Dropbox/GLM Inference/van de geer-confidence regions for high-dimensional models.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Central limit theorem,Generalized linear model,Lasso,Linear model,Multiple testing,Semiparametric efficiency,Sparsity},
number = {3},
pages = {1166--1202},
title = {On asymptotically optimal confidence regions and tests for high-dimensional models},
volume = {42},
year = {2014}
}
@article{JM15,
abstract = {Performing statistical inference in high-dimensional models is an outstanding challenge. A major source of difficulty is the absence of precise information on the distribution of high-dimensional regularized estimators. Here, we consider linear regression in the high-dimensional regime {\$}p\backslashgg n{\$} and the Lasso estimator. In this context, we would like to perform inference on a high-dimensional parameters vector {\$}\backslashtheta{\^{}}*\backslashin R{\^{}}p{\$}. Important progress has been achieved in computing confidence intervals and p-values for single coordinates {\$}\backslashtheta{\^{}}*{\_}i{\$}, {\$}i\backslashin \backslash{\{}1,\backslashdots,p\backslash{\}}{\$}. A key role in these new inferential methods is played by a certain de-biased (or de-sparsified) estimator {\$}\backslashwidehat{\{}\backslashtheta{\}}{\^{}}d{\$} that is constructed from the Lasso estimator. Earlier work establishes that, under suitable assumptions on the design matrix, the coordinates of {\$}\backslashwidehat{\{}\backslashtheta{\}}{\^{}}d{\$} are asymptotically Gaussian provided the true parameters vector {\$}\backslashtheta{\^{}}*{\$} is {\$}s{\_}0{\$}-sparse with {\$}s{\_}0 = o(\backslashsqrt{\{}n{\}}/\backslashlog p ){\$}. The condition {\$}s{\_}0 = o(\backslashsqrt{\{}n{\}}/ \backslashlog p ){\$} is considerably stronger than the one required for consistent estimation, namely {\$}s{\_}0 = o(n/ \backslashlog p ){\$}. Here we consider Gaussian designs with known or unknown population covariance. When the covariance is known, we prove that the de-biased estimator is asymptotically Gaussian under the nearly optimal condition {\$}s{\_}0 = o(n/ (\backslashlog p){\^{}}2){\$}. Note that $\backslash$emph{\{}earlier work was limited to {\$}s{\_}0 = o(\backslashsqrt{\{}n{\}}/ \backslashlog p){\$} even for perfectly known covariance.{\}} The same conclusion holds if the population covariance is unknown but can be estimated sufficiently well, e.g. because its inverse is very sparse. For intermediate regimes, we describe the trade-off between sparsity in the coefficients {\$}\backslashtheta{\^{}}*{\$}, and sparsity in the inverse covariance of the design.},
journal = {arXiv},
arxivId = {1508.02757},
author = {Javanmard, A. and Montanari, A.},
eprint = {1508.02757},
file = {:Users/saili/Dropbox/bootstrap/Bootstrap in high-dimensional problems/JM15-Debiasing the Lasso.pdf:pdf},
pages = {1--32},
title = {{De-biasing the Lasso: Optimal Sample Size for Gaussian Designs}},
url = {http://arxiv.org/abs/1508.02757},
year = {2015}
}
@article{CL10,
abstract = {In this article, we derive the asymptotic distribution of the bootstrapped Lasso estimator of the regression parameter in a multiple linear regression model. It is shown that under some mild regularity conditions on the design vectors and the regularization parameter, the bootstrap approximation converges weakly to a random measure. The convergence result rigorously establishes a previously known heuristic formula for the limit distribution of the bootstrapped Lasso estimator. It is also shown that when one or more components of the regression parameter vector are zero, the bootstrap may fail to be consistent.},
author = {Chatterjee, A. and Lahiri, S.},
doi = {10.1090/S0002-9939-2010-10474-4},
issn = {0002-9939},
journal = {Proceedings of the American Mathematical Society},
keywords = {and phrases,bootstrap,c 2010 american mathematical,consistency,dms-0707139,penalized regression,random measure,society,supported by nsf grant,this research was partially},
number = {12},
pages = {4497--4509},
title = {{Asymptotic properties of the residual bootstrap for lasso estimators}},
url = {http://www.ams.org/journals/proc/2010-138-12/S0002-9939-2010-10474-4/S0002-9939-2010-10474-4.pdf},
volume = {138},
year = {2010}
}
@article{KF00,
author = {Knight, K. and Fu, W.},
file = {:Users/saili/Dropbox/bootstrap/Bootstrap in high-dimensional problems/Asymptotics of Lasso-type estimators.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {1356--1378},
title = {{ASYMPTOTICS FOR LASSO-TYPE ESTIMATORS By Keith Knight 1 and Wenjiang Fu 2}},
volume = {28},
year = {2000}
}

@article{Wain09,
abstract = {The problem of consistently estimating the sparsity pattern of a vector beta* isin Rp based on observations contaminated by noise arises in various contexts, including signal denoising, sparse approximation, compressed sensing, and model selection. We analyze the behavior of l1-constrained quadratic programming (QP), also referred to as the Lasso, for recovering the sparsity pattern. Our main result is to establish precise conditions on the problem dimension p, the number k of nonzero elements in beta*, and the number of observations n that are necessary and sufficient for sparsity pattern recovery using the Lasso. We first analyze the case of observations made using deterministic design matrices and sub-Gaussian additive noise, and provide sufficient conditions for support recovery and linfin-error bounds, as well as results showing the necessity of incoherence and bounds on the minimum value. We then turn to the case of random designs, in which each row of the design is drawn from a N (0, Sigma) ensemble. For a broad class of Gaussian ensembles satisfying mutual incoherence conditions, we compute explicit values of thresholds 0 {\textless} thetasl(Sigma) les thetasu(Sigma) {\textless} +infin with the following properties: for any delta {\textgreater} 0, if n {\textgreater} 2 (thetasu + delta) klog (p- k), then the Lasso succeeds in recovering the sparsity pattern with probability converging to one for large problems, whereas for n {\textless} 2 (thetasl - delta)klog (p - k), then the probability of successful recovery converges to zero. For the special case of the uniform Gaussian ensemble (Sigma = Iptimesp), we show that thetasl = thetas{\textless}u = 1, so that the precise threshold n = 2 klog(p- k) is exactly determined.},
archivePrefix = {arXiv},
arxivId = {math/0605740},
author = {Wainwright, Martin J.},
doi = {10.1109/TIT.2009.2016018},
eprint = {0605740},
isbn = {- 0018-9448},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {???1-constraints,Compressed sensing,Convex relaxation,High-dimensional inference,Model selection,Phase transitions,Signal denoising,Sparse approximation,Subset selection},
mendeley-groups = {Debiased Lasso},
number = {5},
pages = {2183--2202},
primaryClass = {math},
title = {{Sharp thresholds for high-dimensional and noisy sparsity recovery using $\ell_1$-constrained quadratic programming (Lasso)}},
volume = {55},
year = {2009}
}

@article{ZY06,
abstract = {Sparsity or parsimony of statistical models is crucial for their proper interpretations, as in sciences and social sciences. Model selection is a commonly used method to find such models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani, 1996) is now being used as a computationally feasible alternative to model selection. Therefore it is important to study Lasso for model selection purposes. In this paper, we prove that a single condition, which we call the Irrepresentable Condition, is almost necessary and sufficient for Lasso to select the true model both in the classical fixed p setting and in the large p setting as the sample size n gets large. Based on these results, sufficient conditions that are verifiable in practice are given to relate to previous works and help applications of Lasso for feature selection and sparse representation. This Irrepresentable Condition, which depends mainly on the covariance of the predictor variables, states that Lasso selects the true model consistently if and (almost) only if the predictors that are not in the true model are "irrepresentable" (in a sense to be clarified) by predictors that are in the true model. Furthermore, simulations are carried out to provide insights and understanding of this result.},
archivePrefix = {arXiv},
arxivId = {1305.7477},
author = {Zhao, Peng and Yu, Bin},
doi = {10.1109/TIT.2006.883611},
eprint = {1305.7477},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {consistency,lasso,model selection,regularization,sparsity},
pages = {2541--2563},
title = {{On model selection consistency of Lasso}},
url = {http://dl.acm.org/citation.cfm?id=1248637},
volume = {7},
year = {2006}
}

@article{Deze16,
abstract = {We propose a residual and wild bootstrap methodology for individual and simultaneous inference in high-dimensional linear models with possibly non-Gaussian and heteroscedastic errors. We establish asymptotic consistency for simultaneous inference for parameters in groups {\$}G{\$}, where {\$}p \backslashgg n{\$}, {\$}s{\_}0 = o(n{\^{}}{\{}1/2{\}}/\backslash{\{}\backslashlog(p) \backslashlog(|G|){\^{}}{\{}1/2{\}}\backslash{\}}){\$} and {\$}\backslashlog(|G|) = o(n{\^{}}{\{}1/7{\}}){\$}, with {\$}p{\$} the number of variables, {\$}n{\$} the sample size and {\$}s{\_}0{\$} denoting the sparsity. The theory is complemented by many empirical results. Our proposed procedures are implemented in the R-package hdi.},
archivePrefix = {arXiv},
arxivId = {1606.03940},
author = {Dezeure, Ruben and B{\"{u}}hlmann, Peter and Zhang, Cun-Hui},
eprint = {1606.03940},
journal = {arXiv preprint arXiv:1606.03940},
keywords = {de-biased lasso,de-sparsified lasso,dimensional linear model,gaussian approximation for maxima,heteroscedastic errors,high-,multiple testing,westfall-young method},
mendeley-groups = {Debiased Lasso},
title = {High-dimensional simultaneous inference with the bootstrap},
url = {http://arxiv.org/abs/1606.03940},
year = {2016}
}

@article{Vers10,
abstract = {This is a tutorial on some basic non-asymptotic methods and concepts in random matrix theory. The reader will learn several tools for the analysis of the extreme singular values of random matrices with independent rows or columns. Many of these methods sprung off from the development of geometric functional analysis since the 1970's. They have applications in several fields, most notably in theoretical computer science, statistics and signal processing. A few basic applications are covered in this text, particularly for the problem of estimating covariance matrices in statistics and for validating probabilistic constructions of measurement matrices in compressed sensing. These notes are written particularly for graduate students and beginning researchers in different areas, including functional analysts, probabilists, theoretical statisticians, electrical engineers, and theoretical computer scientists.},
archivePrefix = {arXiv},
arxivId = {1011.3027},
author = {Vershynin, Roman},
doi = {10.1017/CBO9780511794308.006},
eprint = {1011.3027},
file = {:Users/saili/Dropbox/Reference/Non asymptotic properties for random matrices.pdf:pdf},
isbn = {9781107005587},
mendeley-groups = {Debiased Lasso},
pages = {210--268},
title = {{Introduction to the non-asymptotic analysis of random matrices}},
url = {http://arxiv.org/abs/1011.3027},
year = {2010}
}

@book{VB11,
abstract = {Modern statistics deals with large and complex data sets, and consequently with models containing a large number of parameters. This book presents a detailed account of recently developed approaches, including the Lasso and ...},
author = {B{\"{u}}hlmann, Peter and van de Geer, Sara},
booktitle = {Journal of Applied Statistics},
doi = {10.1080/02664763.2012.694258},
isbn = {9783642201929},
issn = {0266-4763},
mendeley-groups = {Debiased Lasso},
pages = {xvii + 556},
title = {{Statistics for high-dimensional data: Methods, Theory and Applications}},
url = {http://www.springer.com/statistics/statistical+theory+and+methods/book/978-3-642-20191-2},
year = {2011}
}


@article{ZC17,
author = {Xianyang Zhang and Guang Cheng},
title = {Simultaneous Inference for High-Dimensional Linear Models},
journal = {Journal of the American Statistical Association},
volume = {0},
number = {0},
pages = {1-12},
year = {2017},
doi = {10.1080/01621459.2016.1166114},

URL = { 
        http://dx.doi.org/10.1080/01621459.2016.1166114
    
},
eprint = { 
        http://dx.doi.org/10.1080/01621459.2016.1166114
    
}
}


